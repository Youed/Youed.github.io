<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="2017.6 CS硕士毕业,目前就职于中国电信IT研发中心大数据部门"><title>DeepLearning——Word2vec笔记 | CaoK' Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DeepLearning——Word2vec笔记</h1><a id="logo" href="/.">CaoK' Blog</a><p class="description">But you didn't</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DeepLearning——Word2vec笔记</h1><div class="post-meta">Nov 2, 2017</div><div class="post-content"><h2 id="常见的统计语言模型"><a href="#常见的统计语言模型" class="headerlink" title="常见的统计语言模型"></a>常见的统计语言模型</h2><p>参考：有道的Deep Learning 实战之 word2vec<br><strong>1. 上下文无关模型</strong><br><strong>2. n-gram</strong><br><strong>3. n-pos</strong><br> n-pos 只是 n-gram 的一种衍生模型。n-gram 模型假定第t个词出现概率条件依赖它前N-1个词，而现实中很多词出现的概率是条件依赖于它前面词的语法功能的。n-pos模型就是基于这种假设的模型，它将词按照其语法功能进行分类，由这些词类决定下一个词出现的概率。这样的词类称为词性（Part-of-Speech，简称为 POS）<br><strong>4. 基于决策树的语言模型</strong><br><strong>5. 最大熵模型</strong><br><strong>6. 自适应模型</strong><br>前面的模型概率分布都是预先从训练语料库中估算好的，属于<strong>静态语言模型</strong>。而自适应语言模型类似是 Online Learning 的过程，即根据少量新数据动态调整模型，属于动态模型。 在自然语言中，经常出现这样现象：某些在文本中通常很少出现的词，在某一局部文本中突然大量地出现。能够根据词在局部文本中出现的情况动态地调整语言模型中的概率分布数据的语言模型成为动态、自适应或者基于缓存的语言模型。 通常的做法是将静态模型与动态模型通过参数融合到一起，这种混合模型可以有效地避免数据稀疏的问题。</p>
<a id="more"></a>
<h2 id="g-gram模型"><a href="#g-gram模型" class="headerlink" title="g-gram模型"></a>g-gram模型</h2><p>传统的统计语言模型是表示语言基本单位（一般为句子） 的概率分布函数，这个概率分布也就是该语言的生成模型。一般语言模型可以使用各个词语条件概率的形式表示</p>
<p><img src="https://i.imgur.com/GYZMvgQ.png" alt=""><br><img src="https://i.imgur.com/V6zqbLl.png" alt=""><br>$$p(s)=p(w_1^T)=p(w_1,w_2,\ldots,w<em>T)=\prod</em>{t=1}^Tp(w_t|Context)$$</p>
<p><strong>关于n的取值</strong><br>一般来说，$n$的取值需要同时考虑计算复杂度和模型效果2个方面。如表1所示：（词典大小为20000）<br><img src="https://i.imgur.com/9Qcqg7Q.png" alt=""><br>另外，n-gram模型还需要<strong>平滑化</strong>的环节</p>
<p><strong>hierarchical softmax本质</strong>是把 N 分类问题变成 log(N)次二分类<br><strong>negative sampling本质</strong>是预测总体类别的一个子集</p>
<p><strong>Python Word2Vec参数内容</strong><br>用gensim函数库训练Word2Vec模型有很多配置参数。这里对gensim文档的Word2Vec函数的参数说明进行翻译。</p>
<pre><code>class gensim.models.word2vec.Word2Vec(sentences=None,size=100,alpha=0.025,window=5, min_count=5, max_vocab_size=None, sample=0.001,seed=1, workers=3,min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=&lt;built-in function hash&gt;,iter=5,null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)
</code></pre><p>参数：</p>
<p>· sentences：可以是一个·ist，对于大语料集，建议使用BrownCorpus,Text8Corpus或·ineSentence构建。<br>· sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。<br>· size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。<br>· window：表示当前词与预测词在一个句子中的最大距离是多少<br>· alpha: 是学习速率<br>· seed：用于随机数发生器。与初始化词向量有关。<br>· min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5<br>· max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。<br>· sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)<br>· workers参数控制训练的并行数。<br>· hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。<br>· negative: 如果&gt;0,则会采用negativesamp·ing，用于设置多少个noise words<br>· cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。<br>· hashfxn： hash函数来初始化权重。默认使用python的hash函数<br>· iter： 迭代次数，默认为5<br>· trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。<br>· sorted_vocab： 如果为1（defau·t），则在分配word index 的时候会先对单词基于频率降序排序。<br>· batch_words：每一批的传递给线程的单词的数量，默认为10000<br><strong>相关优质资料</strong></p>
<p>有道：Deep Learning 实战之 word2vec<br>word2vec 中的数学原理详解：<a href="http://blog.csdn.net/itplus/article/details/37969817" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/37969817</a>（强推）<br>秒懂词向量Word2vec的本质：<a href="https://mp.weixin.qq.com/s/aeoFx6sIX6WNch51XRF5sg" target="_blank" rel="external">https://mp.weixin.qq.com/s/aeoFx6sIX6WNch51XRF5sg</a><br>Xin Rong 的论文：『word2vec Parameter Learning Explained』(需理解原理看这个就够了)</p>
</div><div class="tags"><a href="/tags/DeepLearning/">DeepLearning</a><a href="/tags/Word2vec/">Word2vec</a></div><div class="post-nav"><a class="pre" href="/2017/11/20/Hive数据倾斜问题/">Hive数据倾斜问题以及解决方法</a><a class="next" href="/2017/10/02/神经网络与深度学习阅读笔记/">神经网络与深度学习</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Github/" style="font-size: 15px;">Github</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Mechine-Learning/" style="font-size: 15px;">Mechine Learning</a> <a href="/tags/Decision-Tree/" style="font-size: 15px;">Decision Tree</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/URL解析/" style="font-size: 15px;">URL解析</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/数据清洗/" style="font-size: 15px;">数据清洗</a> <a href="/tags/URL，数据挖掘/" style="font-size: 15px;">URL，数据挖掘</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Pyspark/" style="font-size: 15px;">Pyspark</a> <a href="/tags/Windows/" style="font-size: 15px;">Windows</a> <a href="/tags/Xgboost/" style="font-size: 15px;">Xgboost</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/hdfs/" style="font-size: 15px;">hdfs</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/Word2vec/" style="font-size: 15px;">Word2vec</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/03/09/URL挖掘微信QQ账号实例说明/">URL挖掘微信/QQ账号并关联Hive入库实例</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/29/URL提取互联网账号初步解析/">URL提取互联网账号初步解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/URL提取Email之后的数据清洗 /">URL提取Email之后的数据清洗</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/11/spark中读取hdfs文件/">spark中读取hdfs文件的几个问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/20/Hive数据倾斜问题/">Hive数据倾斜问题以及解决方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/02/word2vec入门/">DeepLearning——Word2vec笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/02/神经网络与深度学习阅读笔记/">神经网络与深度学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/28/Ubuntu（16.04)  anaconda安装tensorflow/">Ubuntu（16.04)  anaconda安装tensorflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/21/install xgboost in Anaconda Python (Windows platform)/">install xgboost in Anaconda Python (Windows platform)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/14/install pyspark on windows/">在windows上面安装并用jupyter运行pyspark</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.csdn.net/" title="CSDN" target="_blank">CSDN</a><ul></ul><a href="http://spark.apache.org/docs/latest/" title="Spark" target="_blank">Spark</a><ul></ul><a href="https://stackoverflow.com/questions" title="stackoverflow" target="_blank">stackoverflow</a><ul></ul><a href="https://zh.wikipedia.org/" title="Wiki" target="_blank">Wiki</a><ul></ul><a href="https://tech.meituan.com/archives" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://data.qq.com/blog" title="腾讯大数据" target="_blank">腾讯大数据</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">CaoK' Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>