<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="2017.6 CS硕士毕业,目前就职于中国电信IT研发中心大数据部门"><title>神经网络与深度学习 | CaoK' Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">神经网络与深度学习</h1><a id="logo" href="/.">CaoK' Blog</a><p class="description">But you didn't</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">神经网络与深度学习</h1><div class="post-meta">Oct 2, 2017</div><div class="post-content"><p>备注1：本文为Michael Nielsen的《Neural Networks and Deep Learning》的阅读笔记。<br>备注2：本文所贴链接均为个人觉得写得不错的blog<br>备注3：在看此文之前对ML一些基础知识有所了解那真是极好的<br>备注4：这本书其实是一本入门书，我第一遍阅读大概花了2-3天，私以为第一次看不用纠结于其中冗余的公式与证明</p>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>1.感知机的模型与学习过程<br>注：可以参考李航的统计学习方法：<a href="http://blog.csdn.net/qll125596718/article/details/8394186" target="_blank" rel="external">http://blog.csdn.net/qll125596718/article/details/8394186</a><br>2.感知机是如何实现与非门等基本运算的<br>3.其他：感知机与其他线性模型的对比：</p>
<a id="more"></a>
<p>参考我之前写的一篇小结：<a href="https://shimo.im/doc/PYXLO1pB96kwCHL5" target="_blank" rel="external">https://shimo.im/doc/PYXLO1pB96kwCHL5</a></p>
<h2 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h2><p>1.定义与图形<br>2.为什么要用这种形式的激活函数？<br>其实我们只是在用S型神经元来模拟阶跃函数。微积分告诉我们：<br><img src="https://i.imgur.com/pEpIpZP.png" alt=""><br>也就是说，∆output 是⼀个反映权重和偏置变化 —— 即 ∆wj 和 ∆b  的线性函数，<br><strong>当我们使⽤⼀个不同的激活函数，最⼤的变化是上面公式中⽤于偏导数的特定值的改变。（这点很重要，接下来会说明在神经网络里S型神经元为什么不是一个好的激活函数)</strong><br>S型神经元与感知机最大的不同，它可以输出0到1之间的任何实数，这非常有用，当我们想要输出来表⽰⼀个神经⽹络的图像像素输⼊的平均强度。</p>
<h2 id="一个简单的分类手写数字的网络（MNIST数据集）"><a href="#一个简单的分类手写数字的网络（MNIST数据集）" class="headerlink" title="一个简单的分类手写数字的网络（MNIST数据集）"></a>一个简单的分类手写数字的网络（MNIST数据集）</h2><p>1.为什么最后的输出选择了10个神经元而不是16个或者其他？<br>备注一下<br>参考书P12的解释（解释的很好）<br>2.二次代价函数与一般梯度下降法<br>关于梯度下降法，可以参考<a href="http://blog.csdn.net/njucp/article/details/50488869" target="_blank" rel="external">http://blog.csdn.net/njucp/article/details/50488869</a><br>关于牛顿法与拟牛顿法，可以参考<a href="http://blog.csdn.net/itplus/article/details/21896619" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/21896619</a><br>备注：牛顿法最终必需去计算 C 的⼆阶偏导，这代价可是⾮常⼤的。为了理解为什么这种做法代价⾼，假设我们想求所有的⼆阶偏导。如果我们有上百万的变量 vj，那我们必须要计算数万亿（即百万次的平⽅）级别的⼆阶偏导！这会造成很⼤的计算代价。不过也有⼀些避免这类问题的技巧，寻找梯度下降算法的替代品也是个很活跃的研究领域（如拟牛顿法）<br>3.随机梯度下降法<br>备注：本书关于随机梯度讲的很好，可以重点参考P19<br>其思想就是通过随机选取⼩量训练输⼊样本来计算 ∇Cx，进⽽估算梯度 ∇C。<br>随机梯度下降通过随机地选取并训练输⼊的⼩批量数据来⼯作，其中梯度是在当前⼩批量数据中的所有训练样本 Xj 上进⾏的。然后我们再挑选另⼀随机选定的⼩批量数据去训练。直到我们⽤完了所有的训练输⼊，这被称为完成了⼀个训练迭代期（epoch） 。然后我们就会开始⼀个新的训练迭代期。<br>4.源码(这部分可以好好看书P21，很好理解)<br><code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git</code><br>备注：在理解源码之前强烈推荐看下一个BP的实例，可以非常清楚的理解BP算法工作流。<br><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank" rel="external">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a><br>核心方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span></span></div><div class="line"><span class="function"><span class="title">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span></span></div><div class="line"><span class="function"><span class="title">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,test_data=None)</span></span></div><div class="line"><span class="function"><span class="title">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span></span></div><div class="line"><span class="function"><span class="title">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span></span></div></pre></td></tr></table></figure></p>
<p>超参数：训练的迭代期数量，⼩批量数据⼤⼩和学习速率<br>对于一些问题：<br>   <strong> 复杂的算法 ≤ 简单的学习算法 + 好的训练数据</strong></p>
<h2 id="改进的神经网络的学习方法（核心章节"><a href="#改进的神经网络的学习方法（核心章节" class="headerlink" title="改进的神经网络的学习方法（核心章节)"></a>改进的神经网络的学习方法（核心章节)</h2><p><em>question1</em>:在哪种层⾯上，反向传播是快速的算法？<br>我们需要链式法则来计算梯度，有办法改善吗？比如用<br><img src="https://i.imgur.com/OuAY7SV.png" alt=""><br>来计算梯度，这真的是一个好方法吗？遗憾的是，当你实现了之后，运⾏起来这样的⽅法⾮常缓慢。为了理解原因，想象我们的⽹络中有 1000000 权重。对每个不同的权重 wj 我们需要计算<br><img src="https://i.imgur.com/pu9t4k6.png" alt=""><br>来计算。这意味着为了计算梯度，我们需要计算代价函数 1 000000 次，需要 1000000 前向传播（对每个样本）。我们同样需要计算 C(w)，总共是⼀次⽹络传播需要 1000001 次。反向传播聪明的地⽅就是它确保我们可以同时计算所有的偏导数 ，仅仅使⽤⼀次前向传播，加上⼀次后向传播。<br><strong>交叉熵代价函数：一种解决学习缓慢的方法</strong><br>问题：当使用sigmoid函数+二次代价函数时，发现当错误比较明显时，参数学习的变化却并不大（偏导数），与人类的认知矛盾。<br>因此引入交叉代价函数：<br><img src="https://i.imgur.com/fPHDnu9.png" alt=""><br>特性：<br>1.C&gt;0<br>2.在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低<br>3.偏导数：<br><img src="https://i.imgur.com/MgUCXv0.png" alt=""><br>也就是说，<em>权重学习的速度受到 σ(z) - y，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。避免了像在⼆次代价函数中类似⽅程中 σ′(z) 导致的学习缓慢</em><br>4.如果是多元的输出，定义交叉熵如下<br><img src="https://i.imgur.com/8JGzTuy.png" alt=""><br><strong>softmax：另外一种解决学习缓慢的方法</strong><br><img src="https://i.imgur.com/gfuffyi.png" alt=""><br><img src="https://i.imgur.com/XBCSkRx.png" alt=""><br>其中，上式为第j个神经元的激活值。<br>我们看到柔性最⼤值层的输出是⼀些相加为 1 正数的集合。换⾔之，柔性最⼤值层的输出可以被看做<br>是⼀个概率分布。<br>softmax的2个属性：<br>1，单调性：增加zj会提升对用的输出激活函数值而降低其他输出激活值<br>2，非局部性：任何特定的输出激活值都依赖所有的带权输入。<br>softmax一般与log代价函数绑定，经过推导。可以得到：<br><img src="https://i.imgur.com/u2i7LAu.png" alt=""><br>所以，实际中，我们应该使⽤⼀个具有交叉熵代价的 S 型输出层，还是⼀个具有对数似然<br>代价的柔性最⼤值输出层呢？实际上，在很多应⽤场景中，这两种⽅式的效果都不错。<br><strong>过度拟合与规范化</strong><br><em>为何要使⽤ validation_data 来替代 test_data 防⽌过度拟合问题？</em><br>实际上，这是⼀个更为⼀般的策略的⼀部分，这个⼀般的策略就是使⽤ validation_data 来衡量不同的超参数（如迭代期，学习速率，最好的⽹络架构等等）的选择的效果。<br><em>为何⽤ validation_data 取代 test_data 来设置更好的超参数？</em><br>为了理解这点，想想当设置超参数时，我们想要尝试许多不同的超参数选择。如果我们<br>设置超参数是基于 test_data 的话，可能最终我们就会得到过度拟合于test_data 的超参数。也就<br>是说，我们可能会找到那些符合test_data 特点的超参数，但是⽹络的性能并不能够泛化到其他<br>数据集合上。我们借助 validation_data 来克服这个问题。然后⼀旦获得了想要的超参数，最终<br>我们就使⽤ test_data 进⾏准确率测量。这给了我们在test_data 上的结果是⼀个⽹络泛化能⼒真<br>正的度量⽅式的信⼼。换⾔之，你可以将验证集看成是⼀种特殊的训练数据集能够帮助我们学<br>习好的超参数。这种寻找好的超参数的⽅法有时候被称为 hold out ⽅法，因为 validation_data<br>是从traning_data 训练集中留出或者“拿出”的⼀部分。<br><em>怎么防止过拟合？</em><br>1，增加数据。实际训练数据是宝贵的资源，不可行<br>2.降低网络的规模。然而，⼤的⽹络拥有⼀种⽐⼩⽹络更强的潜⼒，所以这⾥存在⼀种应⽤冗余性的选项。<br>3.规范化<br><img src="https://i.imgur.com/pXd3xOQ.png" alt=""><br>规范化后的学习规则变化<br>注意：L2范式没有限制偏置。<br><img src="https://i.imgur.com/TExxcFV.png" alt=""><br><img src="https://i.imgur.com/UfCejfd.png" alt=""><br>所以也叫<em>权值衰减</em><br><em>为什么规范化可以减轻过度拟合</em><br>通常的说法是：⼩的权重在某种程度上，意味着更低的复杂性，也就对数据给出了⼀种更简单却更强⼤解释，因此应该优先选择===&gt;<strong>奥卡姆剃刀原则</strong><br><strong>一些其他的规范化技术</strong><br>L1范式<br>关于L1与L2的对比，可以参考<br><a href="https://shimo.im/doc/WdgNMkQTAT00lgXx" target="_blank" rel="external">https://shimo.im/doc/WdgNMkQTAT00lgXx</a><br>很重要<br>此外，L1范式由于在某一些点不可导，所以不能直接用梯度下降，一般使用最小角回归算法<br>弃权<br>从随机（临时）地删除⽹络中的⼀半的隐藏神经元开始，同时让输⼊层和输出层的神经元保持不变。<br><img src="https://i.imgur.com/HMpnC1u.png" alt=""><br><strong>为什么有用</strong>？弃权作者的解释：因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机⼦集中更加健壮的特征。<br>”换⾔之，如果我们将我们的神经⽹络看做⼀个进⾏预测的模型的话，我们就可以将弃权看做是⼀种确保模型对于⼀部分证据丢失健壮的⽅式。这样看来，弃权和 L1、 L2 规范化也是有相似之处的，这也倾向于更⼩的权重，最后让⽹络对丢失个体连接的场景更加健壮。<br><em>人为扩展训练数据</em><br>比如旋转<br><strong>权重初始化（P85，讲的很精彩）</strong><br>之前的⽅式就是根据独⽴⾼斯随机变量来选择权重和偏置，其被归⼀化为均值为 0，标准差 1。现在我们看看是否能寻找⼀些更好的⽅式来设置初始的权重和偏置，这也许能帮<br>助我们的⽹络学习得更快<br>===》使⽤均值为 0 标准差为<br>[\frac{1}{\sqrt[n]}]<br> 的⾼斯随机分布初始化这些权重。关于这么改为什么有用，原书有个例子，很清楚</p>
<h2 id="超参数的选择（重要）"><a href="#超参数的选择（重要）" class="headerlink" title="超参数的选择（重要）"></a>超参数的选择（重要）</h2><p>需要一些启发式的想法：<br><strong>宽泛策略：</strong><br>开始可以将模型简单化，并能够在短时间间隔看到输出，从而调参<br><strong>学习速率：</strong><br>这个与其他机器学习模型类似。可以0.01，0.5……往上升，直到你找到⼀个 η 的值使得在开始若⼲回合代价就开始震荡或者增加 。<br>显然， η 实际值不应该⽐阈值⼤。实际上，如果 η 的值重复使⽤很多回合的话，你更应该使<br>⽤稍微⼩⼀点的值，例如，阈值的⼀半这样的选择。这样的选择能够允许你训练更多的回合，不<br>会减慢学习的速度。如果分类准确率在⼀段时间内不再提升的时候提前终⽌<br><strong>小批量数据大小：</strong><br>取值1，相当于在线学习，选择最好的⼩批量数据⼤⼩也是⼀种折衷。太⼩了，你不会⽤上很好的矩阵库的快速计算。太⼤，你是不能够⾜够频繁地更新权重的。<br>⼩批量数据⼤⼩的选择其实是相对独⽴的⼀个超参数（⽹络整体架构外的参数），所以你不需要优化那些参数来寻找好的⼩批量数据⼤⼩。因此，可以选择的⽅式就是使⽤某些可以接受的值（不需要是最优的）作为其他参数的选择，然后进⾏不同⼩批量数据⼤⼩的尝试，像上⾯那样调整 η。<br><strong>自动技术：</strong><br>grid search<br><strong>总结：</strong><br>超参数优化就不是⼀个已经被完全解决的问题<br>实践中，超参数之间存在着很多关系。你可能使⽤ η 进⾏试验，发现效果不错，然后去优化 λ，发现这⾥⼜和 η 混在⼀起了。在实践中，⼀般是来回往复进⾏的，最终逐步地选择到好的值。总之，启发式规则其实都是经验，不是⾦规⽟律。你应该注意那些没有效果的尝试的信号，然后乐于尝试更多试验。特别地，这意味着需要更加细致地监控神经⽹络的⾏为，特别是验证集上的准确率。<br><strong>其他技术：</strong><br>牛顿法的变形：参考<a href="http://blog.csdn.net/itplus/article/details/21896619" target="_blank" rel="external">http://blog.csdn.net/itplus/article/details/21896619</a><br><strong>人工神经元的其他模型：</strong><br>tanh神经元<br><img src="https://i.imgur.com/bY5umTI.png" alt=""><br>（输出值域为-1到1，这是和S神经元最大的不同，而BP，梯度下降也可以应用）<br>S神经元的问题：<em>针对同⼀的神经元的所有权重都会或者⼀起增加或者⼀起减少</em>。这就有问题了，因为某些权重可能需要有相反的变化。所以出现了tanh<br>ReLU:<br><img src="https://i.imgur.com/g7abHdi.png" alt=""><br>回想起 sigmoid 神经元在饱和时停⽌学习的问题，也就是输出接近 0 或者 1 的时候。<br>Tanh 神经元也有类似的问题。<br>对⽐⼀下，提⾼ ReLU 带权输⼊并不会导致其饱和，所以就不存在前⾯那样的学习速度下降。另外，当带权输⼊是负数的时候，梯度就消失了，所以神经元就完全停⽌了学习。这就是很多有关理解 ReLU 何时何故更优的问题中的两个。<br><em>梯度爆炸与梯度消失</em><br>参考P142，讲的挺好的。<br>核心：梯度表达式<br><img src="https://i.imgur.com/lPOJ2RR.png" alt=""></p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>可以快速过一遍，科普用，主要讲了CNN，具体的实现细节与原理需看其他书籍，讲的不是很清楚</p>
</div><div class="tags"><a href="/tags/神经网络/">神经网络</a><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a class="pre" href="/2017/11/02/word2vec入门/">DeepLearning——Word2vec笔记</a><a class="next" href="/2017/09/28/Ubuntu（16.04)  anaconda安装tensorflow/">Ubuntu（16.04)  anaconda安装tensorflow</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Github/" style="font-size: 15px;">Github</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Mechine-Learning/" style="font-size: 15px;">Mechine Learning</a> <a href="/tags/Decision-Tree/" style="font-size: 15px;">Decision Tree</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/URL解析/" style="font-size: 15px;">URL解析</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/数据清洗/" style="font-size: 15px;">数据清洗</a> <a href="/tags/URL，数据挖掘/" style="font-size: 15px;">URL，数据挖掘</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Pyspark/" style="font-size: 15px;">Pyspark</a> <a href="/tags/Windows/" style="font-size: 15px;">Windows</a> <a href="/tags/Xgboost/" style="font-size: 15px;">Xgboost</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/hdfs/" style="font-size: 15px;">hdfs</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/Word2vec/" style="font-size: 15px;">Word2vec</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/机器学习，不平衡数据/" style="font-size: 15px;">机器学习，不平衡数据</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/22/Mechine Learning 非平衡数据处理方式与评估/">Mechine Learning 非平衡数据处理方式总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/09/URL挖掘微信QQ账号实例说明/">URL挖掘微信/QQ账号并关联Hive入库实例</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/29/URL提取互联网账号初步解析/">URL提取互联网账号初步解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/URL提取Email之后的数据清洗 /">URL提取Email之后的数据清洗</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/11/spark中读取hdfs文件/">spark中读取hdfs文件的几个问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/20/Hive数据倾斜问题/">Hive数据倾斜问题以及解决方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/02/word2vec入门/">DeepLearning——Word2vec笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/02/神经网络与深度学习阅读笔记/">神经网络与深度学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/28/Ubuntu（16.04)  anaconda安装tensorflow/">Ubuntu（16.04)  anaconda安装tensorflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/21/install xgboost in Anaconda Python (Windows platform)/">install xgboost in Anaconda Python (Windows platform)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.csdn.net/" title="CSDN" target="_blank">CSDN</a><ul></ul><a href="http://spark.apache.org/docs/latest/" title="Spark" target="_blank">Spark</a><ul></ul><a href="https://stackoverflow.com/questions" title="stackoverflow" target="_blank">stackoverflow</a><ul></ul><a href="https://zh.wikipedia.org/" title="Wiki" target="_blank">Wiki</a><ul></ul><a href="https://tech.meituan.com/archives" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://data.qq.com/blog" title="腾讯大数据" target="_blank">腾讯大数据</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">CaoK' Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>