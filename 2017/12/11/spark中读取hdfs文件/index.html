<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="2017.6 CS硕士毕业,目前就职于中国电信IT研发中心大数据部门"><title>spark中读取hdfs文件的几个问题 | CaoK' Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">spark中读取hdfs文件的几个问题</h1><a id="logo" href="/.">CaoK' Blog</a><p class="description">But you didn't</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">spark中读取hdfs文件的几个问题</h1><div class="post-meta">Dec 11, 2017</div><div class="post-content"><p> 使用spark的API读取hdfs的方法是<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(filePath)</div></pre></td></tr></table></figure></p>
<p><strong><em>如果该文件不存在</em></strong>，就会报错，报错多次，就会奔溃<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="type">ERROR</span> <span class="type">JobScheduler</span>: <span class="type">Error</span> running job streaming job <span class="number">1481878392000</span> ms<span class="number">.0</span></div><div class="line">org.apache.hadoop.mapred.<span class="type">InvalidInputException</span>: <span class="type">Input</span> path does not exist: hdfs:<span class="comment">//ns/data/.../4811_000.txt</span></div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.singleThreadedListStatus(<span class="type">FileInputFormat</span>.java:<span class="number">285</span>)</div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.listStatus(<span class="type">FileInputFormat</span>.java:<span class="number">228</span>)</div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.getSplits(<span class="type">FileInputFormat</span>.java:<span class="number">313</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">HadoopRDD</span>.getPartitions(<span class="type">HadoopRDD</span>.scala:<span class="number">199</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>$$anonfun$partitions$<span class="number">2.</span>apply(<span class="type">RDD</span>.scala:<span class="number">239</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>$$anonfun$partitions$<span class="number">2.</span>apply(<span class="type">RDD</span>.scala:<span class="number">237</span>)</div><div class="line">	at scala.<span class="type">Option</span>.getOrElse(<span class="type">Option</span>.scala:<span class="number">120</span>)</div><div class="line">	...</div><div class="line"><span class="type">ERROR</span> <span class="type">ApplicationMaster</span>: <span class="type">User</span> <span class="class"><span class="keyword">class</span> <span class="title">threw</span> <span class="title">exception</span></span>: org.apache.hadoop.mapred.<span class="type">InvalidInputException</span>: <span class="type">Input</span> path does not exist: hdfs:<span class="comment">//ns/data...00.txt</span></div><div class="line">org.apache.hadoop.mapred.<span class="type">InvalidInputException</span>: <span class="type">Input</span> path does not exist: hdfs:<span class="comment">//ns/data/hjpt/...000.txt</span></div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.singleThreadedListStatus(<span class="type">FileInputFormat</span>.java:<span class="number">285</span>)</div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.listStatus(<span class="type">FileInputFormat</span>.java:<span class="number">228</span>)</div><div class="line">	at org.apache.hadoop.mapred.<span class="type">FileInputFormat</span>.getSplits(<span class="type">FileInputFormat</span>.java:<span class="number">313</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">HadoopRDD</span>.getPartitions(<span class="type">HadoopRDD</span>.scala:<span class="number">199</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>$$anonfun$partitions$<span class="number">2.</span>apply(<span class="type">RDD</span>.scala:<span class="number">239</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>$$anonfun$partitions$<span class="number">2.</span>apply(<span class="type">RDD</span>.scala:<span class="number">237</span>)</div><div class="line">	at scala.<span class="type">Option</span>.getOrElse(<span class="type">Option</span>.scala:<span class="number">120</span>)</div><div class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>.partitions(<span class="type">RDD</span>.scala:<span class="number">237</span>)</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>由于spark中hdfs读是lazy的，所以无法使用try-catch把它装住，即使用try-catch将其包住也会报错。<br>所以目前使用的解决方法是需要在读取该文件之前检验该文件是否存在。<br>在spark中的实现不需要再次指定hadoopConf，只要从sc中拿就可以了：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> conf = sc.hadoopConfiguration</div><div class="line"><span class="keyword">val</span> fs = org.apache.hadoop.fs.<span class="type">FileSystem</span>.get(conf)</div><div class="line"><span class="keyword">val</span> exists = fs.exists(<span class="keyword">new</span> org.apache.hadoop.fs.<span class="type">Path</span>(<span class="string">"/path/on/hdfs/to/SUCCESS.txt"</span>))</div></pre></td></tr></table></figure></p>
<p>实际实现如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = eachRdd.sparkContext</div><div class="line"><span class="keyword">val</span> hadoopConf: <span class="type">Configuration</span> = sc.hadoopConfiguration</div><div class="line"><span class="keyword">val</span> fs: <span class="type">FileSystem</span> = org.apache.hadoop.fs.<span class="type">FileSystem</span>.get(hadoopConf)</div><div class="line"><span class="comment">// 这里是否不需要collect？</span></div><div class="line"><span class="keyword">val</span> lines: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">FtpMap</span>)] = oiddRdd.collect()</div><div class="line"><span class="comment">// 文件名流转化为文件数据流</span></div><div class="line">lines.foreach &#123;</div><div class="line">eachFileJson: (<span class="type">String</span>, <span class="type">FtpMap</span>) =&gt; &#123;</div><div class="line">  <span class="keyword">val</span> topic: <span class="type">String</span> = eachFileJson._1</div><div class="line">  printLog.info(<span class="string">"topic: "</span> + topic)</div><div class="line">  <span class="keyword">val</span> fileJson = eachFileJson._2</div><div class="line">  <span class="keyword">val</span> filePath = fileJson.file_path</div><div class="line">  <span class="keyword">val</span> fileExists: <span class="type">Boolean</span> = <span class="keyword">try</span> &#123;</div><div class="line">    fs.exists(<span class="keyword">new</span> org.apache.hadoop.fs.<span class="type">Path</span>(filePath))</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; &#123;</div><div class="line">      printLog.error(<span class="string">"Exception: filePath:"</span> + filePath + <span class="string">" e:"</span> + e)</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (fileExists) &#123;</div><div class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(filePath)</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>其他</strong><br>如果读取一个hdfs目录下的所有文件，当文件的数目非常多，比如说有一亿个文件，由于spark读hdfs文件是lazy的，它在读取hdfs下该文件的列表的时候，会先将这个列表保存到内存中，但是如果在这期间hdfs文件被删除了，则还是会发生文件不存在的错误！</p>
</div><div class="tags"><a href="/tags/Spark/">Spark</a><a href="/tags/hdfs/">hdfs</a></div><div class="post-nav"><a class="pre" href="/2017/12/18/URL提取Email之后的数据清洗 /">URL提取Email之后的数据清洗</a><a class="next" href="/2017/11/20/Hive数据倾斜问题/">Hive数据倾斜问题以及解决方法</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Pyspark/" style="font-size: 15px;">Pyspark</a> <a href="/tags/Github/" style="font-size: 15px;">Github</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Mechine-Learning/" style="font-size: 15px;">Mechine Learning</a> <a href="/tags/Decision-Tree/" style="font-size: 15px;">Decision Tree</a> <a href="/tags/数据挖掘/" style="font-size: 15px;">数据挖掘</a> <a href="/tags/URL解析/" style="font-size: 15px;">URL解析</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/数据清洗/" style="font-size: 15px;">数据清洗</a> <a href="/tags/URL，数据挖掘/" style="font-size: 15px;">URL，数据挖掘</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/Anaconda/" style="font-size: 15px;">Anaconda</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/Jupyter/" style="font-size: 15px;">Jupyter</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Windows/" style="font-size: 15px;">Windows</a> <a href="/tags/Xgboost/" style="font-size: 15px;">Xgboost</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/hdfs/" style="font-size: 15px;">hdfs</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/Word2vec/" style="font-size: 15px;">Word2vec</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/机器学习，不平衡数据/" style="font-size: 15px;">机器学习，不平衡数据</a> <a href="/tags/atec/" style="font-size: 15px;">atec</a> <a href="/tags/python/" style="font-size: 15px;">python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/07/12/atec初赛总结/">atec初赛第九名小结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/22/Mechine Learning 非平衡数据处理方式与评估/">Mechine Learning 非平衡数据处理方式总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/09/URL挖掘微信QQ账号实例说明/">URL挖掘微信/QQ账号并关联Hive入库实例</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/29/URL提取互联网账号初步解析/">URL提取互联网账号初步解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/18/URL提取Email之后的数据清洗 /">URL提取Email之后的数据清洗</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/11/spark中读取hdfs文件/">spark中读取hdfs文件的几个问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/20/Hive数据倾斜问题/">Hive数据倾斜问题以及解决方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/02/word2vec入门/">DeepLearning——Word2vec笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/02/神经网络与深度学习阅读笔记/">神经网络与深度学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/28/Ubuntu（16.04)  anaconda安装tensorflow/">Ubuntu（16.04)  anaconda安装tensorflow</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.csdn.net/" title="CSDN" target="_blank">CSDN</a><ul></ul><a href="http://spark.apache.org/docs/latest/" title="Spark" target="_blank">Spark</a><ul></ul><a href="https://stackoverflow.com/questions" title="stackoverflow" target="_blank">stackoverflow</a><ul></ul><a href="https://zh.wikipedia.org/" title="Wiki" target="_blank">Wiki</a><ul></ul><a href="https://tech.meituan.com/archives" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://data.qq.com/blog" title="腾讯大数据" target="_blank">腾讯大数据</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">CaoK' Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>